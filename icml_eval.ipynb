{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135a23f6-07a4-4f97-b1c4-5e6fd9b56802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from music21 import converter, midi\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea686560-e7e3-4b3a-96cb-a4a50d54a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/mnt/sdb/ibukey/asap-dataset/Bach/Fugue/bwv_848')\n",
    "performance_wav1 = str(path/'Denisova06M.wav')\n",
    "performance_wav2 = str(path/'Lee01M.wav')\n",
    "musicxml_score = str(path/'xml_score.musicxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1668a7da-b56a-420a-9449-2dada4d2b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file_list.txt\", \"w\") as file:\n",
    "    file.write(performance_wav1.replace('/mnt/sdb/ibukey/', '/opt/')+'\\n')\n",
    "    file.write(performance_wav2.replace('/mnt/sdb/ibukey/', '/opt/')+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08aff548-5bae-4eee-ae3a-aab6a06277a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio to MIDI transcription\n",
    "os.makedirs('midi', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23ac6a15-bfe7-4c80-a6f3-2649892e50f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.01kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.7-slim-bullse  0.1s\n",
      "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (12/14)                                        docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.01kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.7-slim-bullse  0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/python:3.7-slim-bullseye@sha256:603879  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 6.04kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt update && apt install -y     vim     build-ess  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN pip install magenta                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] RUN git clone https://github.com/tensorflow/magenta.gi  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN curl https://storage.googleapis.com/magentadata/mo  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN  cd /opt && unzip /opt/maestro_checkpoint.zip       0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/10] COPY onsets_frames_transcription_transcribe.py /opt/magenta/m  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/10] COPY file_list.txt /opt/file_list.txt                          0.0s\n",
      "\u001b[0m => [ 9/10] COPY transcribe.sh /opt/transcribe.sh                          0.0s\n",
      "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (13/14)                                        docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.01kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.7-slim-bullse  0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/python:3.7-slim-bullseye@sha256:603879  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 6.04kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt update && apt install -y     vim     build-ess  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN pip install magenta                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] RUN git clone https://github.com/tensorflow/magenta.gi  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN curl https://storage.googleapis.com/magentadata/mo  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN  cd /opt && unzip /opt/maestro_checkpoint.zip       0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/10] COPY onsets_frames_transcription_transcribe.py /opt/magenta/m  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/10] COPY file_list.txt /opt/file_list.txt                          0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/10] COPY transcribe.sh /opt/transcribe.sh                          0.1s\n",
      "\u001b[0m => [10/10] RUN chmod +x /opt/transcribe.sh                                0.1s\n",
      "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (13/14)                                        docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.01kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.7-slim-bullse  0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/python:3.7-slim-bullseye@sha256:603879  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 6.04kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt update && apt install -y     vim     build-ess  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN pip install magenta                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] RUN git clone https://github.com/tensorflow/magenta.gi  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN curl https://storage.googleapis.com/magentadata/mo  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN  cd /opt && unzip /opt/maestro_checkpoint.zip       0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/10] COPY onsets_frames_transcription_transcribe.py /opt/magenta/m  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/10] COPY file_list.txt /opt/file_list.txt                          0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/10] COPY transcribe.sh /opt/transcribe.sh                          0.1s\n",
      "\u001b[0m => [10/10] RUN chmod +x /opt/transcribe.sh                                0.2s\n",
      "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (14/14)                                        docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.01kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.7-slim-bullse  0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/python:3.7-slim-bullseye@sha256:603879  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 6.04kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt update && apt install -y     vim     build-ess  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN pip install magenta                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] RUN git clone https://github.com/tensorflow/magenta.gi  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN curl https://storage.googleapis.com/magentadata/mo  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN  cd /opt && unzip /opt/maestro_checkpoint.zip       0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/10] COPY onsets_frames_transcription_transcribe.py /opt/magenta/m  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/10] COPY file_list.txt /opt/file_list.txt                          0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/10] COPY transcribe.sh /opt/transcribe.sh                          0.1s\n",
      "\u001b[0m\u001b[34m => [10/10] RUN chmod +x /opt/transcribe.sh                                0.3s\n",
      "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (15/15) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.01kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.7-slim-bullse  0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/python:3.7-slim-bullseye@sha256:603879  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 6.04kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt update && apt install -y     vim     build-ess  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN pip install magenta                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] RUN git clone https://github.com/tensorflow/magenta.gi  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN curl https://storage.googleapis.com/magentadata/mo  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN  cd /opt && unzip /opt/maestro_checkpoint.zip       0.0s\n",
      "\u001b[0m\u001b[34m => [ 7/10] COPY onsets_frames_transcription_transcribe.py /opt/magenta/m  0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/10] COPY file_list.txt /opt/file_list.txt                          0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/10] COPY transcribe.sh /opt/transcribe.sh                          0.1s\n",
      "\u001b[0m\u001b[34m => [10/10] RUN chmod +x /opt/transcribe.sh                                0.3s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.1s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:dcb8039c108fb1592bb57875350cacc3f1a34cf442d01  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/onf                                     0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! docker build -t onf ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c535353b-5f59-42e9-ac80-3a1a4a20aee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /opt/asap-dataset/Bach/Fugue/bwv_848/Denisova06M.wav\n",
      "2025-01-26 18:13:53.593737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-01-26 18:13:53.593785: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/usr/local/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/usr/local/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/usr/local/lib/python3.7/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "/bin/sh: 1: sox: not found\n",
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2025-01-26 18:14:06.627689: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-01-26 18:14:06.627732: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-01-26 18:14:06.627759: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:458: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W0126 18:14:07.374967 140356763924288 deprecation.py:356] From /usr/local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:458: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /opt/magenta/magenta/models/onsets_frames_transcription/data.py:656: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
      "W0126 18:14:08.666213 140356763924288 deprecation.py:356] From /opt/magenta/magenta/models/onsets_frames_transcription/data.py:656: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
      "WARNING:tensorflow:From /opt/magenta/magenta/models/onsets_frames_transcription/train_util.py:87: The name tf.estimator.tpu.RunConfig is deprecated. Please use tf.compat.v1.estimator.tpu.RunConfig instead.\n",
      "\n",
      "W0126 18:14:09.057233 140356763924288 module_wrapper.py:151] From /opt/magenta/magenta/models/onsets_frames_transcription/train_util.py:87: The name tf.estimator.tpu.RunConfig is deprecated. Please use tf.compat.v1.estimator.tpu.RunConfig instead.\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/opt/train', '_tf_random_seed': None, '_save_summary_steps': 300, '_save_checkpoints_steps': 300, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': None, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=300, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}\n",
      "I0126 18:14:09.058900 140356763924288 estimator.py:202] Using config: {'_model_dir': '/opt/train', '_tf_random_seed': None, '_save_summary_steps': 300, '_save_checkpoints_steps': 300, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': None, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=300, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu False\n",
      "I0126 18:14:09.059626 140356763924288 tpu_context.py:271] _TPUContext: eval_on_tpu False\n",
      "2025-01-26 18:14:09.063580: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-26 18:14:09.090066: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "INFO:tensorflow:Starting transcription for /opt/asap-dataset/Bach/Fugue/bwv_848/Denisova06M.wav...\n",
      "I0126 18:14:09.127884 140356763924288 onsets_frames_transcription_transcribe.py:113] Starting transcription for /opt/asap-dataset/Bach/Fugue/bwv_848/Denisova06M.wav...\n",
      "INFO:tensorflow:Processing file...\n",
      "I0126 18:14:09.128002 140356763924288 onsets_frames_transcription_transcribe.py:119] Processing file...\n",
      "INFO:tensorflow:Running inference...\n",
      "I0126 18:14:09.568819 140356763924288 onsets_frames_transcription_transcribe.py:130] Running inference...\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I0126 18:14:09.803856 140356763924288 estimator.py:1173] Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU/GPU\n",
      "I0126 18:14:09.804101 140356763924288 tpu_estimator.py:3190] Running infer on CPU/GPU\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "WARNING:tensorflow:From /opt/magenta/magenta/contrib/rnn.py:473: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "W0126 18:14:09.911900 140356763924288 deprecation.py:356] From /opt/magenta/magenta/contrib/rnn.py:473: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0126 18:14:09.912171 140356763924288 deprecation.py:356] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "/opt/magenta/magenta/contrib/rnn.py:750: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._names[\"W\"], [input_size + self._num_units, self._num_units * 4])\n",
      "/opt/magenta/magenta/contrib/rnn.py:753: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  initializer=tf.constant_initializer(0.0))\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0126 18:14:11.134199 140356763924288 deprecation.py:356] From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0126 18:14:11.149640 140356763924288 deprecation.py:356] From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I0126 18:14:11.349711 140356763924288 estimator.py:1175] Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0126 18:14:11.504513 140356763924288 monitored_session.py:243] Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /opt/train/model.ckpt\n",
      "I0126 18:14:11.504862 140356763924288 saver.py:1412] Restoring parameters from /opt/train/model.ckpt\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0126 18:14:11.710828 140356763924288 session_manager.py:527] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0126 18:14:11.738026 140356763924288 session_manager.py:530] Done running local_init_op.\n",
      "INFO:tensorflow:Calculating metrics for b'/opt/asap-dataset/Bach/Fugue/bwv_848/Denisova06M.wav' with length 3506\n",
      "I0126 18:14:13.798745 140314786371328 metrics.py:131] Calculating metrics for b'/opt/asap-dataset/Bach/Fugue/bwv_848/Denisova06M.wav' with length 3506\n",
      "INFO:tensorflow:Reference pitches were length 0, returning empty metrics for b'/opt/asap-dataset/Bach/Fugue/bwv_848/Denisova06M.wav':\n",
      "I0126 18:14:13.930218 140314786371328 metrics.py:186] Reference pitches were length 0, returning empty metrics for b'/opt/asap-dataset/Bach/Fugue/bwv_848/Denisova06M.wav':\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I0126 18:14:13.973213 140356763924288 error_handling.py:115] prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I0126 18:14:13.973434 140356763924288 error_handling.py:115] prediction_loop marked as finished\n",
      "INFO:tensorflow:Transcription written to /opt/midi/asap-dataset/Bach/Fugue/bwv_848/Denisova06M.wav.midi.\n",
      "I0126 18:14:14.079857 140356763924288 onsets_frames_transcription_transcribe.py:151] Transcription written to /opt/midi/asap-dataset/Bach/Fugue/bwv_848/Denisova06M.wav.midi.\n",
      "Processing: /opt/asap-dataset/Bach/Fugue/bwv_848/Lee01M.wav\n",
      "2025-01-26 18:14:15.317776: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-01-26 18:14:15.317940: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/usr/local/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/usr/local/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/usr/local/lib/python3.7/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "/bin/sh: 1: sox: not found\n",
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2025-01-26 18:14:27.846353: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-01-26 18:14:27.846389: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-01-26 18:14:27.846413: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:458: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W0126 18:14:28.367902 139786659878720 deprecation.py:356] From /usr/local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:458: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /opt/magenta/magenta/models/onsets_frames_transcription/data.py:656: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
      "W0126 18:14:29.720260 139786659878720 deprecation.py:356] From /opt/magenta/magenta/models/onsets_frames_transcription/data.py:656: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
      "WARNING:tensorflow:From /opt/magenta/magenta/models/onsets_frames_transcription/train_util.py:87: The name tf.estimator.tpu.RunConfig is deprecated. Please use tf.compat.v1.estimator.tpu.RunConfig instead.\n",
      "\n",
      "W0126 18:14:30.136311 139786659878720 module_wrapper.py:151] From /opt/magenta/magenta/models/onsets_frames_transcription/train_util.py:87: The name tf.estimator.tpu.RunConfig is deprecated. Please use tf.compat.v1.estimator.tpu.RunConfig instead.\n",
      "\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/opt/train', '_tf_random_seed': None, '_save_summary_steps': 300, '_save_checkpoints_steps': 300, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': None, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=300, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}\n",
      "I0126 18:14:30.138720 139786659878720 estimator.py:202] Using config: {'_model_dir': '/opt/train', '_tf_random_seed': None, '_save_summary_steps': 300, '_save_checkpoints_steps': 300, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': None, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=300, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu False\n",
      "I0126 18:14:30.139427 139786659878720 tpu_context.py:271] _TPUContext: eval_on_tpu False\n",
      "2025-01-26 18:14:30.144963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-26 18:14:30.180278: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "INFO:tensorflow:Starting transcription for /opt/asap-dataset/Bach/Fugue/bwv_848/Lee01M.wav...\n",
      "I0126 18:14:30.222991 139786659878720 onsets_frames_transcription_transcribe.py:113] Starting transcription for /opt/asap-dataset/Bach/Fugue/bwv_848/Lee01M.wav...\n",
      "INFO:tensorflow:Processing file...\n",
      "I0126 18:14:30.223166 139786659878720 onsets_frames_transcription_transcribe.py:119] Processing file...\n",
      "INFO:tensorflow:Running inference...\n",
      "I0126 18:14:30.775501 139786659878720 onsets_frames_transcription_transcribe.py:130] Running inference...\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I0126 18:14:31.155643 139786659878720 estimator.py:1173] Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU/GPU\n",
      "I0126 18:14:31.155909 139786659878720 tpu_estimator.py:3190] Running infer on CPU/GPU\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "WARNING:tensorflow:From /opt/magenta/magenta/contrib/rnn.py:473: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "W0126 18:14:31.264288 139786659878720 deprecation.py:356] From /opt/magenta/magenta/contrib/rnn.py:473: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0126 18:14:31.264560 139786659878720 deprecation.py:356] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "/opt/magenta/magenta/contrib/rnn.py:750: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._names[\"W\"], [input_size + self._num_units, self._num_units * 4])\n",
      "/opt/magenta/magenta/contrib/rnn.py:753: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  initializer=tf.constant_initializer(0.0))\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0126 18:14:32.469399 139786659878720 deprecation.py:356] From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0126 18:14:32.484744 139786659878720 deprecation.py:356] From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I0126 18:14:32.696334 139786659878720 estimator.py:1175] Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I0126 18:14:32.867713 139786659878720 monitored_session.py:243] Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /opt/train/model.ckpt\n",
      "I0126 18:14:32.868298 139786659878720 saver.py:1412] Restoring parameters from /opt/train/model.ckpt\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I0126 18:14:33.047842 139786659878720 session_manager.py:527] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I0126 18:14:33.075376 139786659878720 session_manager.py:530] Done running local_init_op.\n",
      "INFO:tensorflow:Calculating metrics for b'/opt/asap-dataset/Bach/Fugue/bwv_848/Lee01M.wav' with length 3974\n",
      "I0126 18:14:35.219188 139743245354752 metrics.py:131] Calculating metrics for b'/opt/asap-dataset/Bach/Fugue/bwv_848/Lee01M.wav' with length 3974\n",
      "INFO:tensorflow:Reference pitches were length 0, returning empty metrics for b'/opt/asap-dataset/Bach/Fugue/bwv_848/Lee01M.wav':\n",
      "I0126 18:14:35.364606 139743245354752 metrics.py:186] Reference pitches were length 0, returning empty metrics for b'/opt/asap-dataset/Bach/Fugue/bwv_848/Lee01M.wav':\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I0126 18:14:35.414675 139786659878720 error_handling.py:115] prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I0126 18:14:35.414940 139786659878720 error_handling.py:115] prediction_loop marked as finished\n",
      "INFO:tensorflow:Transcription written to /opt/midi/asap-dataset/Bach/Fugue/bwv_848/Lee01M.wav.midi.\n",
      "I0126 18:14:35.520724 139786659878720 onsets_frames_transcription_transcribe.py:151] Transcription written to /opt/midi/asap-dataset/Bach/Fugue/bwv_848/Lee01M.wav.midi.\n"
     ]
    }
   ],
   "source": [
    "! docker run \\\n",
    "\t-v /home/ibukey/icml_eval/midi:/opt/midi \\\n",
    "\t-v /mnt/sdb/ibukey/asap-dataset:/opt/asap-dataset \\\n",
    "    -t onf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b0358c-6ecd-4205-b5dc-99d8b0c53d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MusicXML to MIDI\n",
    "def musicxml_to_midi(musicxml_file):\n",
    "    score = converter.parse(musicxml_file)\n",
    "    midi_stream = score.write('midi')\n",
    "    print(f\"MIDI file saved as {midi_stream}\")\n",
    "    return midi_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd13cc6a-cfaf-49b1-b8fb-32e46ab07434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align MIDI using DTW\n",
    "def extract_notes(midi_file):\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "    notes = []\n",
    "\n",
    "    for instrument in midi_data.instruments:\n",
    "        for note in instrument.notes:\n",
    "            notes.append((note.start, note.pitch))\n",
    "    \n",
    "    notes.sort(key=lambda x: x[0])\n",
    "    return np.array(notes)\n",
    "\n",
    "def align_midi_with_dtw(midi_file_1, midi_file_2):\n",
    "    notes_1 = extract_notes(midi_file_1)\n",
    "    notes_2 = extract_notes(midi_file_2)\n",
    "    \n",
    "    time_1 = notes_1[:, 0]\n",
    "    pitch_1 = notes_1[:, 1]\n",
    "    \n",
    "    time_2 = notes_2[:, 0]\n",
    "    pitch_2 = notes_2[:, 1]\n",
    "    \n",
    "    # Convert time-pitch sequences into 2D feature matrices\n",
    "    # Feature vector: [time, pitch]\n",
    "    features_1 = np.vstack((time_1, pitch_1))\n",
    "    features_2 = np.vstack((time_2, pitch_2))\n",
    "\n",
    "    # Perform DTW using librosa\n",
    "    D, wp = librosa.sequence.dtw(X=features_1, Y=features_2)\n",
    "    \n",
    "    return D, wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e6f817-89b2-4997-ae9f-03e52673d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alignment(path, midi_file_1, midi_file_2):\n",
    "    # Extract notes from both MIDI files\n",
    "    notes_1 = extract_notes(midi_file_1)\n",
    "    notes_2 = extract_notes(midi_file_2)\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    time_1 = notes_1[:, 0]\n",
    "    pitch_1 = notes_1[:, 1]\n",
    "    time_2 = notes_2[:, 0]\n",
    "    pitch_2 = notes_2[:, 1]\n",
    "    \n",
    "    # Align the notes based on the warping path\n",
    "    aligned_time_1 = [time_1[i] for i, j in path]\n",
    "    aligned_pitch_1 = [pitch_1[i] for i, j in path]\n",
    "    aligned_time_2 = [time_2[j] for i, j in path]\n",
    "    aligned_pitch_2 = [pitch_2[j] for i, j in path]\n",
    "\n",
    "    # Plot the notes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(time_1, pitch_1, color='blue', label='MIDI 1')\n",
    "    plt.scatter(time_2, pitch_2, color='red', label='MIDI 2')\n",
    "    plt.plot(aligned_time_1, aligned_pitch_1, color='blue', linestyle='--', label='Aligned MIDI 1')\n",
    "    plt.plot(aligned_time_2, aligned_pitch_2, color='red', linestyle='--', label='Aligned MIDI 2')\n",
    "    \n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Pitch (MIDI Number)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c71235-4888-4340-ac52-662e965606ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi alignment of two audio recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb5b120-fdfc-4fc6-a576-c7da199faa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_midi1 = 'midi/asap-dataset/Bach/Fugue/bwv_848/Denisova06M.wav.midi'\n",
    "transcribed_midi2 = 'midi/asap-dataset/Bach/Fugue/bwv_848/Lee01M.wav.midi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7077966e-8d3c-4ba4-b2a4-bd487a0ad525",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, wp = align_midi_with_dtw(transcribed_midi1, transcribed_midi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e75a600f-01ef-4007-b39f-364df3a13724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi alignment of synthesized musicxml and audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b44c809-cde8-4402-a316-ba08969ae567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml_to_midi_file = str(musicxml_to_midi(musicxml_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5949d40c-b4f8-4a7a-8bf7-0249e81d2636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D, wp = align_midi_with_dtw(xml_to_midi_file, transcribed_midi1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9477bf-cf13-4640-b22e-64df3683cb79",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ac1e533-8490-425b-a111-6fffdb0df294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mir_eval.transcription import precision_recall_f1_overlap, onset_precision_recall_f1, offset_precision_recall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2da372d3-386c-4e20-9e59-55da09b09dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_data = pretty_midi.PrettyMIDI(transcribed_midi1)\n",
    "ref_intervals, ref_pitches = [], []\n",
    "for instrument in midi_data.instruments:\n",
    "    for note in instrument.notes:\n",
    "        ref_intervals.append((note.start, note.end))\n",
    "        ref_pitches.append(note.pitch)\n",
    "ref_intervals = np.array(ref_intervals)\n",
    "ref_pitches = np.array(ref_pitches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c92a778-ea44-4a40-af4e-451d3a683bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_data = pretty_midi.PrettyMIDI(transcribed_midi2)\n",
    "est_intervals, est_pitches = [], []\n",
    "for instrument in midi_data.instruments:\n",
    "    for note in instrument.notes:\n",
    "        est_intervals.append((note.start, note.end))\n",
    "        est_pitches.append(note.pitch)\n",
    "est_intervals = np.array(est_intervals)\n",
    "est_pitches = np.array(est_pitches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53d09faa-b34f-4d80-96bb-42bdee1643d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_intervals = ref_intervals[wp[::-1].T[0]]\n",
    "ref_pitches = ref_pitches[wp[::-1].T[0]]\n",
    "est_intervals = est_intervals[wp[::-1].T[1]]\n",
    "est_pitches = est_pitches[wp[::-1].T[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84202fc6-378d-4dd5-bec2-cc3a31f524a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f_measure, avg_overlap_ratio = precision_recall_f1_overlap(ref_intervals, ref_pitches, est_intervals, est_pitches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcad4ca2-76db-460c-913b-3dd8962afdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.04 \n",
      "recall: 0.04 \n",
      "f_measure: 0.04 \n",
      "avg_overlap_ratio: 0.74\n"
     ]
    }
   ],
   "source": [
    "print(f\"precision: {round(precision, 2)} \\nrecall: {round(recall, 2)} \\nf_measure: {round(f_measure, 2)} \\navg_overlap_ratio: {round(avg_overlap_ratio, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5baea4df-9573-4160-9b94-348e9a043c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4849665924276169, 0.4849665924276169, 0.4849665924276169)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, r, f = onset_precision_recall_f1(ref_intervals, est_intervals)\n",
    "p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ab59749-e0b6-4b63-853e-a74c73ebdbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5044543429844098, 0.5044543429844098, 0.5044543429844098)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, r, f = offset_precision_recall_f1(ref_intervals, est_intervals)\n",
    "p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e34051-020c-4944-9d02-e64596885537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d29915-ae1c-42b8-b59d-e486df7cb929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844011f-6d92-4c4b-b8af-2e07c715cf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
